[model]
name = unified.prefixtuning
use_description = True
concatenate_description = True
# Should be one of (separate, concatenate, none), we don't use knowledge as well as prefix
knowledge_usage = none
freeze_plm = False
freeze_prefix = False
test = True
prefix_len = True
prefix_use = add

[dataset]
loader_path = ./tasks/macsum_qmsum.py
data_store_path = ./data
upsample_temp = 1

[seq2seq]
constructor = seq2seq_construction.macsum_qmsum

[prefix_tuning]
prefix_sequence_length = 0
mid_dim = 512
prefix_dropout = 0.0

[evaluate]
tool = metrics.rouge.evaluator

[bert]
location = facebook/bart-large-cnn
