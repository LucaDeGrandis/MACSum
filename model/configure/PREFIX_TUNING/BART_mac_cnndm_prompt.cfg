[model]
name = unified.prefixtuning
use_description = True
concatenate_description = True
# Should be one of (separate, concatenate, none), we concat rather than prefix (length = 0)
knowledge_usage = concatenate
freeze_plm = False
freeze_prefix = False
test = True
prefix_len = False
prefix_use = add

[dataset]
loader_path = ./tasks/macsum_cnndm.py
data_store_path = ./data
upsample_temp = 1

[seq2seq]
constructor = seq2seq_construction.macsum_cnndm

[prefix_tuning]
prefix_sequence_length = 0
mid_dim = 512
prefix_dropout = 0.0

[evaluate]
tool = metrics.rouge.evaluator

[bert]
location = facebook/bart-large-cnn
